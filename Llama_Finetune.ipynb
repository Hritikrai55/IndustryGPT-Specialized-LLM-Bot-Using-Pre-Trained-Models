{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hritikrai55/Llama-FineTuning/blob/main/Llama_Finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "\n",
        "Organizations and researchers often possess **domain-specific knowledge** stored in PDF documents.\n",
        "To make this knowledge usable for **intelligent question-answering** and **summarization**,\n",
        "it needs to be integrated into Large Language Models (LLMs).\n",
        "\n",
        "- Pre-trained LLMs (e.g., Llama 3) are powerful but **lack proprietary document knowledge**.  \n",
        "- Traditional fine-tuning of LLMs (8B+ parameters) is **computationally expensive** and often\n",
        "  inaccessible to users with limited GPU resources.  \n",
        "\n",
        "‚û°Ô∏è There is a need for an **efficient method** to adapt LLMs to domain-specific content without requiring large-scale compute.\n"
      ],
      "metadata": {
        "id": "ykWrtX9jW1Rf"
      },
      "id": "ykWrtX9jW1Rf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Business Context\n",
        "\n",
        "Recent advancements in **Large Language Models (LLMs)** have created opportunities for\n",
        "domain-specific AI applications.  \n",
        "\n",
        "- **Llama 3-8B-Instruct** is an open-source LLM offering performance comparable\n",
        "  to proprietary models.  \n",
        "- The challenge is adapting these models to specialized data without costly infrastructure.  \n",
        "\n",
        "**Key enablers:**\n",
        "- **LoRA (Low-Rank Adaptation):** Fine-tunes a small subset of parameters, reducing cost.  \n",
        "- **Quantization (bitsandbytes):** Loads models in **4-bit precision**, lowering VRAM needs from ~32GB to ~8GB.  \n"
      ],
      "metadata": {
        "id": "sBU544AJW2U1"
      },
      "id": "sBU544AJW2U1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objectives\n",
        "\n",
        "## Primary Objectives\n",
        "1. Build an **end-to-end pipeline** for fine-tuning Llama 3-8B-Instruct on PDF documents.  \n",
        "2. Use **parameter-efficient methods (LoRA)** and **4-bit quantization** to run on consumer GPUs.  \n",
        "3. Automate **PDF ingestion, chunking, and dataset creation**.  \n",
        "4. Provide a **web-based interface** for testing and deployment.  \n",
        "\n",
        "## Specific Technical Goals\n",
        "- Extract and preprocess text from PDFs.  \n",
        "- Create **instruction-style datasets** (400 words, 50-word overlap).  \n",
        "- Fine-tune with **LoRA rank=8** on `q_proj` and `v_proj` layers.  \n",
        "- Train using **4-bit quantization** (~75% memory reduction).  \n",
        "- Run for **20 epochs** with gradient accumulation.  \n",
        "- Deploy using a **Gradio-based chat interface**.  \n"
      ],
      "metadata": {
        "id": "eT1lMfZ0W2eM"
      },
      "id": "eT1lMfZ0W2eM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Github Link : https://github.com/Hritikrai55/Llama-FineTuning/tree/main"
      ],
      "metadata": {
        "id": "CxtHWRXP0KE_"
      },
      "id": "CxtHWRXP0KE_"
    },
    {
      "cell_type": "markdown",
      "id": "550a87bf",
      "metadata": {
        "id": "550a87bf"
      },
      "source": [
        "## 1) Install Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0725ddb"
      },
      "source": [
        "Installs the necessary Python libraries for the notebook, including `transformers`, `accelerate`, `datasets`, `peft`, `bitsandbytes`, `safetensors`, `sentencepiece`, `tokenizers` for model handling and training, `pymupdf` for PDF processing, `tqdm` for progress bars, and `gradio` for the user interface. `sentence-transformers` is included for potential synthetic data generation (although not used in this specific flow)."
      ],
      "id": "b0725ddb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4315bcb8",
      "metadata": {
        "id": "4315bcb8"
      },
      "outputs": [],
      "source": [
        "# Install core ML libraries for model training and inference\n",
        "!pip install -q transformers accelerate datasets peft bitsandbytes safetensors sentencepiece tokenizers\n",
        "# Install data processing and UI libraries\n",
        "!pip install -q pymupdf tqdm gradio\n",
        "# Install optional library for synthetic data generation\n",
        "!pip install -q sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5679180",
      "metadata": {
        "id": "e5679180"
      },
      "source": [
        "## 2) Hugging Face Authentication (required for Llama 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59360d59"
      },
      "source": [
        "Handles the Hugging Face login process, which is required to access the Llama 3 model weights. It imports the `login` function from `huggingface_hub` and provides two options for authentication: interactive login (recommended for security) or using a pre-set environment variable for the token."
      ],
      "id": "59360d59"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684225b8",
      "metadata": {
        "id": "684225b8"
      },
      "outputs": [],
      "source": [
        "# Import authentication modules\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# Authenticate with Hugging Face (required for Llama 3 access)\n",
        "# You must accept Meta Llama 3 license on HF model page first\n",
        "login()\n",
        "\n",
        "# Alternative: Use environment variable for token (uncomment if needed)\n",
        "# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"YOUR_TOKEN\"\n",
        "# login(token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86f94a0a",
      "metadata": {
        "id": "86f94a0a"
      },
      "source": [
        "## 3) Configuration Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a592f62"
      },
      "source": [
        "Defines various configuration parameters used throughout the notebook, such as the base model name (`MODEL_NAME`), paths for input PDFs, extracted text, and the generated dataset (`PDF_DIR`, `TEXT_DIR`, `DATA_JSONL`), chunking parameters (`CHUNK_SIZE_WORDS`, `CHUNK_OVERLAP`), and training parameters (`OUTPUT_DIR`, `EPOCHS`, `LEARNING_RATE`, `PER_DEVICE_BATCH_SIZE`, `GRAD_ACCUM`, `MAX_LENGTH`)."
      ],
      "id": "5a592f62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4712e173",
      "metadata": {
        "id": "4712e173"
      },
      "outputs": [],
      "source": [
        "# Import path utilities\n",
        "from pathlib import Path\n",
        "\n",
        "# Base model (Llama 3 8B Instruct). Requires HF access.\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Set up data processing directories\n",
        "PDF_DIR = Path(\"pdfs\")              # Directory containing input PDFs\n",
        "TEXT_DIR = Path(\"texts\")            # Directory for extracted text files\n",
        "DATA_JSONL = Path(\"dataset.jsonl\")  # Output dataset file\n",
        "\n",
        "# Configure text chunking parameters\n",
        "CHUNK_SIZE_WORDS = 400  # Number of words per text chunk\n",
        "CHUNK_OVERLAP = 50      # Word overlap between consecutive chunks\n",
        "\n",
        "# Define training hyperparameters\n",
        "OUTPUT_DIR = Path(\"lora_out\")       # Directory to save LoRA adapters\n",
        "EPOCHS = 20                         # Number of training epochs\n",
        "LEARNING_RATE = 2e-4               # Learning rate for optimization\n",
        "PER_DEVICE_BATCH_SIZE = 1          # Batch size per GPU device\n",
        "GRAD_ACCUM = 8                     # Gradient accumulation steps\n",
        "MAX_LENGTH = 512                   # Maximum token sequence length\n",
        "\n",
        "print(\"Configured.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b825efec",
      "metadata": {
        "id": "b825efec"
      },
      "source": [
        "## 4) PDF Text Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6afccbcc"
      },
      "source": [
        "Extracts text content from PDF files located in the specified `PDF_DIR`. It uses the `fitz` library (part of `pymupdf`) to open and read each PDF page, then joins the page texts, and uses a regular expression to clean up excessive newlines. The extracted text for each PDF is saved as a `.txt` file in the `TEXT_DIR`."
      ],
      "id": "6afccbcc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63691fa1",
      "metadata": {
        "id": "63691fa1"
      },
      "outputs": [],
      "source": [
        "# Import PDF processing and text cleaning libraries\n",
        "import fitz, re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create required directories if they don't exist\n",
        "PDF_DIR.mkdir(exist_ok=True)\n",
        "TEXT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Define function to extract text from a single PDF\n",
        "def extract_text_from_pdf(path: Path) -> str:\n",
        "    # Open PDF document\n",
        "    doc = fitz.open(str(path))\n",
        "    # Extract text from each page\n",
        "    pages = [p.get_text(\"text\") for p in doc]\n",
        "    # Join all pages into single text string\n",
        "    text = \"\\n\".join(pages)\n",
        "    # Remove excessive newlines using regex\n",
        "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Find all PDF files in the input directory\n",
        "pdf_files = list(PDF_DIR.glob(\"*.pdf\"))\n",
        "\n",
        "# Check if PDFs exist and process them\n",
        "if not pdf_files:\n",
        "    print(\"‚ö†Ô∏è Put your PDFs into the 'pdfs/' folder and re-run this cell.\")\n",
        "else:\n",
        "    # Extract text from each PDF with progress bar\n",
        "    for pdf in tqdm(pdf_files, desc=\"Extracting PDFs\"):\n",
        "        # Extract text content\n",
        "        txt = extract_text_from_pdf(pdf)\n",
        "        # Save as text file with same name\n",
        "        (TEXT_DIR / f\"{pdf.stem}.txt\").write_text(txt, encoding=\"utf-8\")\n",
        "    print(\"‚úÖ Text files saved to:\", TEXT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1755c79",
      "metadata": {
        "id": "b1755c79"
      },
      "source": [
        "## 5) Text Chunking & Dataset Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab50435a"
      },
      "source": [
        "Performs the crucial step of chunking the extracted text and creating the instruction-style dataset in JSONL format. It defines a `chunk_text` function that splits the text into smaller chunks with a specified overlap. It then iterates through the text files, chunks the content, and creates a dataset where each chunk is an \"input\" for an instruction (summarization in this case, but the \"output\" is left blank for manual or synthetic filling). The resulting dataset is saved to the `DATA_JSONL` file."
      ],
      "id": "ab50435a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e113ff2c",
      "metadata": {
        "id": "e113ff2c"
      },
      "outputs": [],
      "source": [
        "# Import JSON handling for dataset creation\n",
        "import json\n",
        "\n",
        "# Define function to split text into overlapping chunks\n",
        "def chunk_text(text, chunk_size=CHUNK_SIZE_WORDS, overlap=CHUNK_OVERLAP):\n",
        "    # Split text into individual words\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    # Create chunks with specified overlap\n",
        "    while i < len(words):\n",
        "        # Extract chunk of specified size\n",
        "        chunk_words = words[i: i + chunk_size]\n",
        "        if not chunk_words:\n",
        "            break\n",
        "        # Join words back into text chunk\n",
        "        chunks.append(\" \".join(chunk_words))\n",
        "        # Move to next position with overlap\n",
        "        i += max(1, chunk_size - overlap)\n",
        "    return chunks\n",
        "\n",
        "# Initialize dataset container\n",
        "dataset = []\n",
        "\n",
        "# Find all extracted text files\n",
        "text_files = list(TEXT_DIR.glob(\"*.txt\"))\n",
        "\n",
        "# Process text files if they exist\n",
        "if not text_files:\n",
        "    print(\"‚ö†Ô∏è No .txt files found in 'texts/'. Make sure previous step ran.\")\n",
        "else:\n",
        "    for tf in text_files:\n",
        "        text = tf.read_text(encoding=\"utf-8\")\n",
        "        chunks = chunk_text(text)\n",
        "        # Create training examples for each chunk\n",
        "        for c in chunks:\n",
        "            dataset.append({\n",
        "                \"instruction\": \"Summarize the following passage in 2‚Äì4 concise bullet points.\",\n",
        "                \"input\": c,\n",
        "                \"output\": \"\"\n",
        "            })\n",
        "\n",
        "    # Save dataset in JSONL format (one JSON per line)\n",
        "    with DATA_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for ex in dataset:\n",
        "            f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Created {len(dataset)} examples -> {DATA_JSONL}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b125d9a5",
      "metadata": {
        "id": "b125d9a5"
      },
      "source": [
        "## 6) Dataset Loading & Tokenizer Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa6ffc1f"
      },
      "source": [
        "Loads the dataset created in the previous step using the `load_dataset` function from the `datasets` library. It also loads the tokenizer for the chosen Llama 3 model using `AutoTokenizer`. A `format_example` function is defined to structure the data into the required prompt format for instruction tuning. The dataset is then mapped using this function, and a padding token is added to the tokenizer if it doesn't exist."
      ],
      "id": "fa6ffc1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32fb34f7",
      "metadata": {
        "id": "32fb34f7"
      },
      "outputs": [],
      "source": [
        "# Import dataset and tokenizer libraries\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the JSONL dataset created in previous step\n",
        "data_path = str(DATA_JSONL)\n",
        "print(\"Using dataset:\", data_path)\n",
        "\n",
        "# Load dataset using HuggingFace datasets library\n",
        "ds = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
        "\n",
        "# Define function to format examples for instruction tuning\n",
        "def format_example(example):\n",
        "   # Extract components from each example\n",
        "    instr = example.get(\"instruction\",\"\")\n",
        "    inp = example.get(\"input\",\"\")\n",
        "    out = example.get(\"output\",\"\")\n",
        "    # Format into instruction-tuning template\n",
        "    if inp:\n",
        "        prompt = f\"### Instruction:\\n{instr}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{out}\"\n",
        "    else:\n",
        "        prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{out}\"\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "# Apply formatting to entire datase\n",
        "ds = ds.map(format_example)\n",
        "\n",
        "# Load tokenizer for the specified model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
        "\n",
        "# Add padding token if not present (required for batch processing)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "print(\"Tokenizer ready. Pad token id:\", tokenizer.pad_token_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9772d3e",
      "metadata": {
        "id": "e9772d3e"
      },
      "source": [
        "## 7) Text Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "714f4372"
      },
      "source": [
        "Tokenizes the formatted dataset. The `tokenize` function uses the loaded tokenizer to convert the text prompts into token IDs, truncating sequences longer than `MAX_LENGTH`. The `map` function applies this tokenization in batches, and the original text column is removed."
      ],
      "id": "714f4372"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e722965c",
      "metadata": {
        "id": "e722965c"
      },
      "outputs": [],
      "source": [
        "# Define tokenization function for batch processing\n",
        "def tokenize(batch):\n",
        "  # Convert text to tokens with truncation at max length\n",
        "    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "# Apply tokenization to entire dataset in batches\n",
        "ds_tokenized = ds.map(tokenize, batched=True, remove_columns=ds.column_names)\n",
        "print(ds_tokenized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b5eeb07",
      "metadata": {
        "id": "2b5eeb07"
      },
      "source": [
        "## 8) LoRA Fine-tuning Setup & Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ae1bca6"
      },
      "source": [
        "Here we sets up and runs the LoRA fine-tuning process on the tokenized dataset. It loads the base Llama 3 model in 4-bit precision using `bitsandbytes` for memory efficiency. It prepares the model for k-bit training and configures the LoRA adapters using `LoraConfig`, targeting the `q_proj` and `v_proj` modules. A `DataCollatorForLanguageModeling` is used to prepare the data for training. `TrainingArguments` are defined to configure the training process (epochs, learning rate, batch size, gradient accumulation, etc.). Finally, a `Trainer` is initialized and the `train` method is called to start the fine-tuning. The trained LoRA adapters are saved to the `OUTPUT_DIR`."
      ],
      "id": "8ae1bca6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63bbd243",
      "metadata": {
        "id": "63bbd243"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch and training libraries\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Enable TF32 for faster training on modern GPUs\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "# Load base model with 4-bit quantization for memory efficiency\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    load_in_4bit=True,  # Use 4-bit precision to reduce memory usage\n",
        "    device_map=\"auto\",   # Automatically distribute model across GPUs\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Prepare model for k-bit training (required for quantized models)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configure LoRA (Low-Rank Adaptation) parameters\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                                    # Rank of adaptation matrices\n",
        "    lora_alpha=32,                          # LoRA scaling parameter\n",
        "    target_modules=[\"q_proj\",\"v_proj\"],     # Target attention projection layers\n",
        "    lora_dropout=0.05,                      # Dropout rate for LoRA layers\n",
        "    bias=\"none\",                            # Don't update bias parameters\n",
        "    task_type=\"CAUSAL_LM\"                   # Task type: causal language modeling\n",
        ")\n",
        "\n",
        "# Apply LoRA configuration to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Set up data collator for language modeling (handles padding and batching)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Configure training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(OUTPUT_DIR),                     # Directory to save checkpoints\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,  # Batch size per device\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,        # Steps to accumulate gradients\n",
        "    num_train_epochs=EPOCHS,                       # Number of training epochs\n",
        "    learning_rate=LEARNING_RATE,                   # Learning rate for optimizer\n",
        "    fp16=True,                                     # Use 16-bit precision (bf16 for newer GPUs)\n",
        "    logging_steps=10,                              # Log training metrics every 10 steps\n",
        "    save_strategy=\"epoch\",                         # Save checkpoint every epoch\n",
        "    report_to=\"none\",                              # Disable external logging services\n",
        "    remove_unused_columns=False                    # Keep all dataset columns\n",
        ")\n",
        "\n",
        "# Initialize trainer with model, data, and training configuration\n",
        "trainer = Trainer(\n",
        "    model=model,                    # LoRA-equipped model\n",
        "    args=training_args,             # Training configuration\n",
        "    train_dataset=ds_tokenized,     # Tokenized training dataset\n",
        "    data_collator=data_collator,    # Handles batching and padding\n",
        "    tokenizer=tokenizer             # Tokenizer for text processing\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained LoRA adapters\n",
        "model.save_pretrained(str(OUTPUT_DIR))\n",
        "print(\"‚úÖ LoRA adapters saved to:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0040f124",
      "metadata": {
        "id": "0040f124"
      },
      "source": [
        "## 9) Load Fine-tuned Model & Test Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef6abb69"
      },
      "source": [
        "Now loads the fine-tuned LoRA adapters and demonstrates how to use the model for inference. It loads the base model again and then loads the saved LoRA adapters on top of it using `PeftModel.from_pretrained`. A `generate_response` function is defined to take a prompt, tokenize it, and use the fine-tuned model to generate a response with specified generation parameters (max new tokens, temperature, top_p). A test prompt is then used to demonstrate the generation capability."
      ],
      "id": "ef6abb69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60eb9b74",
      "metadata": {
        "id": "60eb9b74"
      },
      "outputs": [],
      "source": [
        "# Import LoRA model loading utilities\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model again for inference\n",
        "base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, load_in_4bit=True, device_map=\"auto\", trust_remote_code=True)\n",
        "# Load and apply the trained LoRA adapters\n",
        "model = PeftModel.from_pretrained(base, str(OUTPUT_DIR))\n",
        "\n",
        "# Define text generation function with configurable parameters\n",
        "def generate_response(prompt, max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
        "    # Tokenize input prompt and move to GPU\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # Generate response using inference mode (no gradients)\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,      # Maximum tokens to generate\n",
        "            do_sample=True,                     # Enable sampling for diversity\n",
        "            temperature=temperature,            # Control randomness (higher = more creative)\n",
        "            top_p=top_p,                       # Nucleus sampling parameter\n",
        "            pad_token_id=tokenizer.pad_token_id # Handle padding tokens\n",
        "        )\n",
        "    # Decode generated tokens back to text\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Create test prompt in instruction-tuning format\n",
        "test_prompt = (\n",
        "    \"### Instruction:\\nSummarize the following passage in 2‚Äì4 bullet points.\\n\\n\"\n",
        "    \"### Input:\\nArtificial Intelligence is transforming education by enabling personalized learning paths, \"\n",
        "    \"adaptive assessments, and intelligent tutoring systems. Institutions must address data privacy and fairness.\\n\\n\"\n",
        "    \"### Response:\\n\"\n",
        ")\n",
        "\n",
        "# Test the fine-tuned model with sample prompt\n",
        "print(generate_response(test_prompt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2478d7ed",
      "metadata": {
        "id": "2478d7ed"
      },
      "source": [
        "## 10) Gradio Web Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ad63ac2"
      },
      "source": [
        "To test the performance we sets up a Gradio web interface for interacting with the fine-tuned model in a chat-like manner. It defines a `chat_fn` that takes user input and generation parameters, formats the prompt with a system instruction, and uses the `generate_response` function to get the model's output. The Gradio interface is built using `gr.Blocks`, with input and output textboxes, sliders for generation parameters, and a button to trigger the `chat_fn`. The interface is launched locally."
      ],
      "id": "0ad63ac2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d43613",
      "metadata": {
        "id": "c1d43613"
      },
      "outputs": [],
      "source": [
        "# Import Gradio for creating web interface\n",
        "import gradio as gr\n",
        "\n",
        "# Define system instruction for consistent model behavior\n",
        "SYSTEM_INSTR = \"You are a helpful assistant fine-tuned to summarize and answer based on provided input.\"\n",
        "\n",
        "# Define chat function for handling user interactions\n",
        "def chat_fn(user_input, temperature=0.7, top_p=0.9, max_new_tokens=256):\n",
        "    # Format user input into instruction template\n",
        "    prompt = (\n",
        "        f\"### Instruction:\\n{SYSTEM_INSTR}\\n\\n\"\n",
        "        f\"### Input:\\n{user_input}\\n\\n\"\n",
        "        f\"### Response:\\n\"\n",
        "    )\n",
        "    # Generate response using the fine-tuned model\n",
        "    return generate_response(prompt, max_new_tokens=int(max_new_tokens), temperature=float(temperature), top_p=float(top_p))\n",
        "\n",
        "# Create Gradio interface with blocks layout\n",
        "with gr.Blocks() as demo:\n",
        "    # Add title and description\n",
        "    gr.Markdown(\"## ü¶ô Llama 3 LoRA ‚Äî Demo\")\n",
        "    gr.Markdown(\"Enter a passage or question. The model will respond based on its fine-tuned behavior.\")\n",
        "\n",
        "    # Create input textbox for user queries\n",
        "    with gr.Row():\n",
        "        inp = gr.Textbox(label=\"Your input\", lines=8, placeholder=\"Paste a passage or ask a question...\")\n",
        "\n",
        "    # Create parameter controls for generation settings\n",
        "    with gr.Row():\n",
        "        temperature = gr.Slider(0.0, 1.5, value=0.7, step=0.05, label=\"Temperature\")    # Randomness control\n",
        "        top_p = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label=\"Top-p\")              # Vocabulary selection\n",
        "        max_new = gr.Slider(32, 1024, value=256, step=32, label=\"Max new tokens\")      # Response length limit\n",
        "\n",
        "    # Create output textbox for model responses\n",
        "    out = gr.Textbox(label=\"Model output\", lines=12)\n",
        "\n",
        "    # Create generate button\n",
        "    btn = gr.Button(\"Generate\")\n",
        "\n",
        "    # Connect button click to chat function\n",
        "    btn.click(chat_fn, inputs=[inp, temperature, top_p, max_new], outputs=[out])\n",
        "\n",
        "# Launch the web interface\n",
        "demo.launch(share=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c290afd"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "This implementation shows a **practical and accessible way** to fine-tune large LLMs\n",
        "on custom documents.\n",
        "\n",
        "### Key Achievements\n",
        "- **Accessibility:** Train on GPUs with 8-12GB VRAM (vs 32GB+) using LoRA + 4-bit quantization.  \n",
        "- **Efficiency:**  \n",
        "  - ~0.1% parameters updated  \n",
        "  - ~70-80% faster training  \n",
        "  - ~75% lower memory  \n",
        "  - Adapter size: ~30MB (vs multi-GB checkpoints)  \n",
        "- **Practicality:** Automated pipeline from PDF to deployment.  \n",
        "- **Flexibility:** Modular design supports multiple domains, tasks, and parameters.  \n",
        "\n",
        "### Technical Impact\n",
        "- Efficient fine-tuning methods enable **domain adaptation without enterprise resources**.  \n",
        "- Model retains general Llama 3 ability + gains **specialized knowledge**.  \n",
        "\n",
        "### Future Implications\n",
        "- Can be applied across industries:  \n",
        "  - **Legal document analysis**  \n",
        "  - **Medical research summarization**  \n",
        "  - **Technical documentation assistance**  \n",
        "  - **Educational tools**  \n",
        "\n",
        "- **Gradio interface** ensures user-friendly access, bridging **implementation ‚Üí application**.  \n"
      ],
      "id": "9c290afd"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}